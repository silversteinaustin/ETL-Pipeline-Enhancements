# Crowdfunding_ETL

This project demonstrates a streamlined approach to data sanitation through the Extract, Transform, Load (ETL) process, enabling the efficient integration of data into a database. The repository is organized into two main directories: `ETL_Files`, which houses all necessary scripts and resources for executing the ETL process, and `Resources`, containing the raw CSV files utilized in the project.

Leveraging Python, alongside the Pandas library and Python dictionary methods, the raw data is meticulously extracted and transformed within a Jupyter Notebook environment. The project utilizes four CSV files as the foundation for constructing an Entity Relationship Diagram (ERD) and developing a comprehensive table schema. Subsequently, the processed data is seamlessly uploaded into a PostgreSQL database, showcasing a practical application of database management and data integration techniques.

## Project Objective

The main goal of this project was to harness the power of Python and Pandas, along with Python dictionary methods or regular expressions, to build a robust ETL pipeline. My partner and I embarked on this endeavor to practice the critical data engineering skills of extracting, transforming, and loading data. Our journey involved taking raw data and meticulously transforming it within a Jupyter Notebook environment. We aimed to create four CSV files from this processed data, which would then serve as the foundation for constructing an Entity Relationship Diagram (ERD) and developing a comprehensive table schema. The culmination of our project saw these CSV files being uploaded into a PostgreSQL database, showcasing our ability to integrate transformed data into a structured database system efficiently.

This project was not just about executing technical steps; it was a deep dive into the collaborative process of data engineering. We divided the work, ensuring continuous communication and support, which was pivotal for maintaining a unified direction and overcoming challenges. Our efforts were focused on achieving a seamless ETL process that not only served as a testament to our technical skills but also highlighted the importance of teamwork in data projects.

By the end of this project, we aimed to have a fully functional ETL pipeline that could serve as a model for future data integration tasks, demonstrating practical applications of database management and data integration techniques. This endeavor was about more than just fulfilling project requirements; it was about laying the groundwork for advanced data manipulation and analysis skills that I can carry forward into future projects and professional endeavors.


### Key Components

- **Create the Category and Subcategory DataFrames**
- **Create the Campaign DataFrame**
- **Create the Contacts DataFrame**
- **Create the Crowdfunding Database**

## Repository Structure

- `ETL_Files/`: Contains scripts and resources for the ETL process.
- `Resources/`: Contains raw CSV files for the project.
- `ETL_Mini_Project_NRomanoff_JSmith.ipynb`: The main Jupyter notebook for the ETL process.

## Instructions for Interaction

- Explore data transformations and loading through the provided Jupyter Notebook.
- Review CSV exports and ERD for understanding the database schema.
- The processed data can be explored through the PostgreSQL database to see the results of the ETL process.

## Data Sources

- **Crowdfunding data**: Extracted and transformed from provided Excel files.
- **Contacts data**: Managed through either Python dictionary methods or regular expressions.

## Ethical Considerations

This project adheres to ethical standards in data handling, ensuring privacy, accuracy, and transparency throughout the ETL process.

---

*Note: This README has been updated to address feedback and improve clarity and organization. For any questions or further discussions, please reach out.*
